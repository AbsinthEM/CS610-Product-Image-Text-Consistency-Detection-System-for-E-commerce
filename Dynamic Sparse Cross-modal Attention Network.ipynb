{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOsiX3/5V0+Bw3vexbJ4l1K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gsmnpjieB3Iv"},"outputs":[],"source":["# mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load splited data\n","train_path = \"/content/drive/MyDrive/GitHub_Repos/CS610-Product-Image-Text-Consistency-Detection-System-for-E-commerce/amazon_meta_data/split_data/train_data.parquet\"\n","val_path = '/content/drive/MyDrive/GitHub_Repos/CS610-Product-Image-Text-Consistency-Detection-System-for-E-commerce/amazon_meta_data/split_data/val_data.parquet'\n","test_path = '/content/drive/MyDrive/GitHub_Repos/CS610-Product-Image-Text-Consistency-Detection-System-for-E-commerce/amazon_meta_data/split_data/test_data.parquet'\n","\n","train_df = pd.read_parquet(train_path)\n","val_df = pd.read_parquet(val_path)\n","test_df = pd.read_parquet(test_path)"],"metadata":{"id":"EduKXpl4CAzh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer\n","import torchvision.transforms as transforms\n","from PIL import Image\n","import numpy as np\n","import pandas as pd\n","import os\n","import json\n","from tqdm import tqdm\n","\n","def load_sparse_attention_data(input_dir='sparse_attention_features', batch_size=None):\n","    \"\"\"\n","    Load data from files generated by the preprocessing function and recreate the sparse attention data structure.\n","\n","    Parameters:\n","    input_dir: Directory where the preprocessed data is saved.\n","    batch_size: The batch size for data loading. If None, the original saved batch size will be used.\n","\n","    Returns:\n","    A dictionary containing DataLoader, Dataset, and other objects.\n","    \"\"\"\n","    print(f\"Loading sparse attention data from {input_dir}...\")\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"Using device: {device}\")\n","\n","    # === 1. Load dataset information ===\n","    print(\"\\n1. Loading dataset information\")\n","    with open(os.path.join(input_dir, 'dataset_info.json'), 'r') as f:\n","        dataset_info = json.load(f)\n","\n","    max_length = dataset_info['max_length']\n","    bert_model_name = dataset_info['bert_model']\n","    patch_size = dataset_info['patch_size']\n","\n","    if batch_size is None:\n","        batch_size = 32  # Default batch size\n","\n","    # === 2. Load valid indices ===\n","    print(\"\\n2. Loading valid indices\")\n","    train_valid_indices = np.load(os.path.join(input_dir, 'train_valid_indices.npy'))\n","    val_valid_indices = np.load(os.path.join(input_dir, 'val_valid_indices.npy'))\n","    test_valid_indices = np.load(os.path.join(input_dir, 'test_valid_indices.npy'))\n","\n","    # === 3. Load valid DataFrame ===\n","    print(\"\\n3. Loading valid DataFrame\")\n","    train_df_valid = pd.read_csv(os.path.join(input_dir, 'train_df_valid.csv'))\n","    val_df_valid = pd.read_csv(os.path.join(input_dir, 'val_df_valid.csv'))\n","    test_df_valid = pd.read_csv(os.path.join(input_dir, 'test_df_valid.csv'))\n","\n","    # === 4. Initialize BERT tokenizer ===\n","    print(\"\\n4. Initializing BERT tokenizer\")\n","    tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n","\n","    # Add special tokens\n","    special_tokens = ['[TITLE]', '[CAT]', '[FEAT]', '[DESC]', '[DET]']\n","    special_tokens_dict = {'additional_special_tokens': special_tokens}\n","    tokenizer.add_special_tokens(special_tokens_dict)\n","\n","    # === 5. Define Vision Transformer image transformations ===\n","    print(\"\\n5. Defining Vision Transformer image transformations\")\n","    image_transforms = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","    ])\n","\n","    # === 6. Create dataset class ===\n","    print(\"\\n6. Creating dataset class\")\n","    class SparseAttentionDataset(Dataset):\n","        def __init__(self, df, tokenizer, transform, valid_indices, max_length,\n","                     text_column='text_for_deep', image_column='processed_image_path'):\n","            self.df = df\n","            self.tokenizer = tokenizer\n","            self.transform = transform\n","            self.valid_indices = valid_indices\n","            self.max_length = max_length\n","            self.text_column = text_column\n","            self.image_column = image_column\n","\n","            print(f\"Dataset size: {len(self.valid_indices)}\")\n","\n","        def __len__(self):\n","            return len(self.valid_indices)\n","\n","        def __getitem__(self, idx):\n","            orig_idx = self.valid_indices[idx]\n","            row = self.df.iloc[idx]  # Since df is already valid_df, we use idx directly\n","            text = row[self.text_column]\n","            image_path = row[self.image_column]\n","            label = row['is_match']\n","\n","            # Process text - Structured tokenization\n","            field_boundaries = {}\n","\n","            # Find the positions of the field markers\n","            field_markers = ['[TITLE]', '[CAT]', '[FEAT]', '[DESC]', '[DET]']\n","            field_positions = {}\n","\n","            for marker in field_markers:\n","                pos = text.find(marker)\n","                if pos != -1:\n","                    field_positions[marker] = pos\n","\n","            # Sort the field markers by position\n","            sorted_markers = sorted(field_positions.items(), key=lambda x: x[1])\n","\n","            # Extract text for each field\n","            field_texts = {}\n","            for i, (marker, pos) in enumerate(sorted_markers):\n","                start = pos + len(marker)\n","                if i < len(sorted_markers) - 1:\n","                    end = sorted_markers[i+1][1]\n","                else:\n","                    end = len(text)\n","                field_texts[marker] = text[start:end].strip()\n","\n","            # Tokenize each field separately for field-level attention\n","            field_encodings = {}\n","            for marker, field_text in field_texts.items():\n","                if field_text:  # Only process non-empty fields\n","                    field_encodings[marker] = self.tokenizer(\n","                        field_text,\n","                        max_length=max_length // len(field_texts),  # Distribute tokens evenly\n","                        padding='max_length',\n","                        truncation=True,\n","                        return_tensors='pt'\n","                    )\n","\n","            # Merge the tokenized fields, each field has its own token_type_id\n","            merged_input_ids = []\n","            merged_attention_mask = []\n","            merged_token_type_ids = []\n","\n","            field_token_type = 0\n","            field_boundaries = {}\n","\n","            for marker in field_markers:\n","                if marker in field_encodings:\n","                    # Record the start position of the field\n","                    start_pos = len(merged_input_ids)\n","\n","                    # Get the encoding for the field\n","                    field_input_ids = field_encodings[marker]['input_ids'].squeeze()\n","                    field_attention_mask = field_encodings[marker]['attention_mask'].squeeze()\n","\n","                    # Only add non-padding tokens\n","                    valid_length = field_attention_mask.sum().item()\n","                    field_input_ids = field_input_ids[:valid_length]\n","                    field_attention_mask = field_attention_mask[:valid_length]\n","\n","                    # Add to the merged list\n","                    merged_input_ids.extend(field_input_ids.tolist())\n","                    merged_attention_mask.extend(field_attention_mask.tolist())\n","\n","                    # Assign token_type_id for this field\n","                    merged_token_type_ids.extend([field_token_type] * len(field_input_ids))\n","\n","                    # Record the end position of the field\n","                    end_pos = len(merged_input_ids) - 1\n","                    field_boundaries[marker.replace('[', '').replace(']', '')] = (start_pos, end_pos)\n","\n","                    # The next field gets a different token_type_id\n","                    field_token_type += 1\n","\n","            # Ensure the total length doesn't exceed max_length\n","            if len(merged_input_ids) > max_length:\n","                merged_input_ids = merged_input_ids[:max_length]\n","                merged_attention_mask = merged_attention_mask[:max_length]\n","                merged_token_type_ids = merged_token_type_ids[:max_length]\n","\n","            # If the total length is less than max_length, pad to max_length\n","            padding_length = max_length - len(merged_input_ids)\n","            if padding_length > 0:\n","                merged_input_ids.extend([self.tokenizer.pad_token_id] * padding_length)\n","                merged_attention_mask.extend([0] * padding_length)\n","                merged_token_type_ids.extend([0] * padding_length)\n","\n","            # Convert to tensors\n","            input_ids = torch.tensor(merged_input_ids)\n","            attention_mask = torch.tensor(merged_attention_mask)\n","            token_type_ids = torch.tensor(merged_token_type_ids)\n","\n","            # Process image (using patches)\n","            try:\n","                image = Image.open(image_path).convert('RGB')\n","                image_tensor = self.transform(image)\n","            except Exception as e:\n","                print(f\"Error processing image {image_path}: {e}\")\n","                # Return a zero tensor as a placeholder\n","                image_tensor = torch.zeros((3, 224, 224))\n","\n","            return {\n","                'input_ids': input_ids,\n","                'attention_mask': attention_mask,\n","                'token_type_ids': token_type_ids,\n","                'field_boundaries': field_boundaries,\n","                'image': image_tensor,\n","                'label': torch.tensor(label, dtype=torch.long),\n","                'idx': torch.tensor(idx)\n","            }\n","\n","    # === 7. Create datasets ===\n","    print(\"\\n7. Creating datasets\")\n","    train_dataset = SparseAttentionDataset(\n","        train_df_valid, tokenizer, image_transforms,\n","        train_valid_indices, max_length\n","    )\n","\n","    val_dataset = SparseAttentionDataset(\n","        val_df_valid, tokenizer, image_transforms,\n","        val_valid_indices, max_length\n","    )\n","\n","    test_dataset = SparseAttentionDataset(\n","        test_df_valid, tokenizer, image_transforms,\n","        test_valid_indices, max_length\n","    )\n","\n","    # Custom collate function to handle field_boundaries\n","    def sparse_attention_collate_fn(batch):\n","        input_ids = torch.stack([item['input_ids'] for item in batch])\n","        attention_mask = torch.stack([item['attention_mask'] for item in batch])\n","        token_type_ids = torch.stack([item['token_type_ids'] for item in batch])\n","        images = torch.stack([item['image'] for item in batch])\n","        labels = torch.stack([item['label'] for item in batch])\n","        indices = torch.stack([item['idx'] for item in batch])\n","\n","        # field_boundaries is a list of dictionaries, cannot be simply stacked\n","        field_boundaries = [item['field_boundaries'] for item in batch]\n","\n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'token_type_ids': token_type_ids,\n","            'field_boundaries': field_boundaries,\n","            'image': images,\n","            'label': labels,\n","            'idx': indices\n","        }\n","\n","    # === 8. Create data loaders ===\n","    print(\"\\n8. Creating data loaders\")\n","    train_loader = DataLoader(\n","        train_dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=2,\n","        collate_fn=sparse_attention_collate_fn,\n","        pin_memory=True if torch.cuda.is_available() else False\n","    )\n","\n","    val_loader = DataLoader(\n","        val_dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=2,\n","        collate_fn=sparse_attention_collate_fn,\n","        pin_memory=True if torch.cuda.is_available() else False\n","    )\n","\n","    test_loader = DataLoader(\n","        test_dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=2,\n","        collate_fn=sparse_attention_collate_fn,\n","        pin_memory=True if torch.cuda.is_available() else False\n","    )\n","\n","    print(\"\\nSparse attention data loading complete!\")\n","\n","    return {\n","        'train_loader': train_loader,\n","        'val_loader': val_loader,\n","        'test_loader': test_loader,\n","        'train_dataset': train_dataset,\n","        'val_dataset': val_dataset,\n","        'test_dataset': test_dataset,\n","        'tokenizer': tokenizer,\n","        'image_transforms': image_transforms,\n","        'train_df_valid': train_df_valid,\n","        'val_df_valid': val_df_valid,\n","        'test_df_valid': test_df_valid,\n","        'dataset_info': dataset_info\n","    }\n","\n","# Call the preprocessing function\n","sparse_attention_data = preprocess_for_sparse_attention(\n","    train_df=train_df,\n","    val_df=val_df,\n","    test_df=test_df,\n","    text_column='text_for_deep',\n","    image_column='processed_image_path',\n","    output_dir='/content/drive/MyDrive/GitHub_Repos/CS610-Product-Image-Text-Consistency-Detection-System-for-E-commerce/amazon_meta_data/sparse_attention_features',\n","    bert_model_name='bert-base-uncased',\n","    max_length=256,\n","    batch_size=16,\n","    patch_size=16\n",")\n","\n","# Get the returned data loaders and datasets\n","train_loader = sparse_attention_data['train_loader']\n","val_loader = data_dict['val_loader']\n","test_loader = data_dict['test_loader']\n","tokenizer = sparse_attention_data['tokenizer']\n","\n","# Print the dataset sizes\n","print(f\"Training set size: {len(sparse_attention_data['train_dataset'])} valid samples\")\n","print(f\"Validation set size: {len(sparse_attention_data['val_dataset'])} valid samples\")\n","print(f\"Test set size: {len(sparse_attention_data['test_dataset'])} valid samples\")"],"metadata":{"id":"doz2sGRVCChQ"},"execution_count":null,"outputs":[]}]}