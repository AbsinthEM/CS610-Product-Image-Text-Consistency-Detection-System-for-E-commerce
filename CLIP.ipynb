{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNQYQlFXLyfj+BNUW+Svr6B"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"naWquS76CUio"},"outputs":[],"source":["# mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load splited data\n","train_path = \"/content/drive/MyDrive/GitHub_Repos/CS610-Product-Image-Text-Consistency-Detection-System-for-E-commerce/amazon_meta_data/split_data/train_data.parquet\"\n","val_path = '/content/drive/MyDrive/GitHub_Repos/CS610-Product-Image-Text-Consistency-Detection-System-for-E-commerce/amazon_meta_data/split_data/val_data.parquet'\n","test_path = '/content/drive/MyDrive/GitHub_Repos/CS610-Product-Image-Text-Consistency-Detection-System-for-E-commerce/amazon_meta_data/split_data/test_data.parquet'\n","\n","train_df = pd.read_parquet(train_path)\n","val_df = pd.read_parquet(val_path)\n","test_df = pd.read_parquet(test_path)"],"metadata":{"id":"bxy0NcGECbbn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install git+https://github.com/openai/CLIP.git"],"metadata":{"id":"7wRWU6AyCcoa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","from PIL import Image\n","import numpy as np\n","import pandas as pd\n","import os\n","import json\n","from tqdm import tqdm\n","import clip\n","\n","def preprocess_for_clip(train_df, val_df, test_df,\n","                       text_column='text_for_deep',\n","                       image_column='processed_image_path',\n","                       output_dir='clip_features',\n","                       clip_model_name=\"ViT-B/16\",\n","                       batch_size=16):\n","    \"\"\"\n","    Prepare text and image data for CLIP model\n","\n","    Parameters:\n","    train_df, val_df, test_df: Pre-split datasets\n","    text_column: Column name for text data\n","    image_column: Column name for image paths\n","    output_dir: Directory to save the features\n","    clip_model_name: Name of the CLIP model to use (ViT-B/32, ViT-B/16, RN50, RN101, etc.)\n","    batch_size: Batch size for processing\n","    \"\"\"\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"Using device: {device}\")\n","\n","    print(f\"Preparing data for CLIP ({clip_model_name}) model...\")\n","\n","    # === 1. Load CLIP model and preprocessors ===\n","    print(\"\\n1. Loading CLIP model and preprocessor\")\n","    model, preprocess = clip.load(clip_model_name, device=device)\n","\n","    # Extract tokenizer for text processing\n","    tokenizer = lambda text: clip.tokenize(text, truncate=True)\n","\n","    # === 2. Create dataset class ===\n","    class CLIPDataset(Dataset):\n","        def __init__(self, df, preprocess, tokenizer, text_column, image_column):\n","            self.df = df\n","            self.preprocess = preprocess\n","            self.tokenizer = tokenizer\n","            self.text_column = text_column\n","            self.image_column = image_column\n","\n","            # Record valid indices\n","            self.valid_indices = []\n","            for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Validating image paths\"):\n","                if pd.notna(row[image_column]) and os.path.exists(row[image_column]):\n","                    self.valid_indices.append(idx)\n","\n","            print(f\"Valid samples: {len(self.valid_indices)}/{len(df)}\")\n","\n","        def __len__(self):\n","            return len(self.valid_indices)\n","\n","        def __getitem__(self, idx):\n","            orig_idx = self.valid_indices[idx]\n","            row = self.df.iloc[orig_idx]\n","            text = row[self.text_column]\n","            image_path = row[self.image_column]\n","            label = row['is_match']\n","\n","            # Process text - use CLIP's tokenizer\n","            # Note: Actual tokenization will happen during batching; here, we return the raw text\n","            text_for_tokenization = text\n","\n","            # Process image - use CLIP's preprocessor\n","            try:\n","                image = Image.open(image_path).convert('RGB')\n","                image_tensor = self.preprocess(image)\n","            except Exception as e:\n","                print(f\"Error processing image {image_path}: {e}\")\n","                # Return a zero tensor as a placeholder\n","                image_tensor = torch.zeros((3, 224, 224))\n","\n","            return {\n","                'text': text_for_tokenization,\n","                'image': image_tensor,\n","                'label': torch.tensor(label, dtype=torch.long),\n","                'idx': torch.tensor(orig_idx)\n","            }\n","\n","    # === 3. Create datasets ===\n","    print(\"\\n2. Creating datasets\")\n","    train_dataset = CLIPDataset(\n","        train_df, preprocess, tokenizer,\n","        text_column, image_column\n","    )\n","\n","    val_dataset = CLIPDataset(\n","        val_df, preprocess, tokenizer,\n","        text_column, image_column\n","    )\n","\n","    test_dataset = CLIPDataset(\n","        test_df, preprocess, tokenizer,\n","        text_column, image_column\n","    )\n","\n","    # Custom collate function to handle CLIP's batching\n","    def clip_collate_fn(batch):\n","        texts = [item['text'] for item in batch]\n","        images = torch.stack([item['image'] for item in batch])\n","        labels = torch.stack([item['label'] for item in batch])\n","        indices = torch.stack([item['idx'] for item in batch])\n","\n","        # Tokenize text in batches\n","        text_tokens = tokenizer(texts)\n","\n","        return {\n","            'text': texts,          # Raw text for debugging and visualization\n","            'text_tokens': text_tokens,  # Tokenized text\n","            'image': images,\n","            'label': labels,\n","            'idx': indices\n","        }\n","\n","    # === 4. Create data loaders ===\n","    print(\"\\n3. Creating data loaders\")\n","    train_loader = DataLoader(\n","        train_dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=2,\n","        collate_fn=clip_collate_fn,\n","        pin_memory=True if torch.cuda.is_available() else False\n","    )\n","\n","    val_loader = DataLoader(\n","        val_dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=2,\n","        collate_fn=clip_collate_fn,\n","        pin_memory=True if torch.cuda.is_available() else False\n","    )\n","\n","    test_loader = DataLoader(\n","        test_dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=2,\n","        collate_fn=clip_collate_fn,\n","        pin_memory=True if torch.cuda.is_available() else False\n","    )\n","\n","    # === 5. Save dataset processing information ===\n","    print(\"\\n4. Saving dataset information\")\n","    dataset_info = {\n","        'train_size': len(train_dataset),\n","        'val_size': len(val_dataset),\n","        'test_size': len(test_dataset),\n","        'clip_model': clip_model_name,\n","    }\n","\n","    with open(os.path.join(output_dir, 'dataset_info.json'), 'w') as f:\n","        json.dump(dataset_info, f)\n","\n","    # Save valid indices\n","    np.save(os.path.join(output_dir, 'train_valid_indices.npy'), np.array(train_dataset.valid_indices))\n","    np.save(os.path.join(output_dir, 'val_valid_indices.npy'), np.array(val_dataset.valid_indices))\n","    np.save(os.path.join(output_dir, 'test_valid_indices.npy'), np.array(test_dataset.valid_indices))\n","\n","    # Save valid DataFrames\n","    train_df_valid = train_df.iloc[train_dataset.valid_indices].copy()\n","    val_df_valid = val_df.iloc[val_dataset.valid_indices].copy()\n","    test_df_valid = test_df.iloc[test_dataset.valid_indices].copy()\n","\n","    train_df_valid.to_csv(os.path.join(output_dir, 'train_df_valid.csv'), index=False)\n","    val_df_valid.to_csv(os.path.join(output_dir, 'val_df_valid.csv'), index=False)\n","    test_df_valid.to_csv(os.path.join(output_dir, 'test_df_valid.csv'), index=False)\n","\n","    print(\"\\nCLIP model data preparation complete!\")\n","\n","    return {\n","        'train_loader': train_loader,\n","        'val_loader': val_loader,\n","        'test_loader': test_loader,\n","        'train_dataset': train_dataset,\n","        'val_dataset': val_dataset,\n","        'test_dataset': test_dataset,\n","        'model': model,\n","        'preprocess': preprocess,\n","        'tokenizer': tokenizer,\n","        'train_df_valid': train_df_valid,\n","        'val_df_valid': val_df_valid,\n","        'test_df_valid': test_df_valid\n","    }"],"metadata":{"id":"7uUtzmwpCf5q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import clip\n","\n","clip_data = preprocess_for_clip(\n","    train_df=train_df,\n","    val_df=val_df,\n","    test_df=test_df,\n","    text_column='text_for_deep',\n","    image_column='processed_image_path',\n","    output_dir='/content/drive/MyDrive/GitHub_Repos/CS610-Product-Image-Text-Consistency-Detection-System-for-E-commerce/amazon_meta_data/clip_features',\n","    clip_model_name=\"ViT-B/16\",  # Optional: \"ViT-B/32\", \"RN50\", \"RN101\"\n","    batch_size=16\n",")\n","\n","# Print the dataset sizes\n","print(f\"Training set size: {len(clip_data['train_dataset'])} valid samples\")\n","print(f\"Validation set size: {len(clip_data['val_dataset'])} valid samples\")\n","print(f\"Test set size: {len(clip_data['test_dataset'])} valid samples\")"],"metadata":{"id":"i5ic3O__CnWJ"},"execution_count":null,"outputs":[]}]}